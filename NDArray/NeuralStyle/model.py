import mxnet as mximport mxnet.ndarray as ndimport mxnet.autograd as autogradfrom matplotlib import pyplot as pltfrom data_preprocessing import data_preprocessingfrom tqdm import *import urllibimport osdef artistic_Image(noise_image,image_size):    image = noise_image.reshape((-1,)+image_size)    r,g,b = nd.split(image ,axis=0, num_outputs=3)    #Denormalization by JG    r= nd.multiply(r,0.229)+0.485    g= nd.multiply(g,0.224)+0.456    b= nd.multiply(b,0.225)+0.406    image=nd.concat(r,g,b,dim=0)    '''    matplotlib supports float32 and uint8 data types. For grayscale, matplotlib supports only float32.     If your array data does not meet one of these descriptions, you need to rescale it.    '''    image = nd.transpose(image, axes=(1, 2, 0))    image = nd.clip(image , a_min=0 , a_max=1)    image = nd.multiply(image,255)    image = nd.clip(image, a_min=0, a_max=255).astype('uint8')    plt.imshow(image.asnumpy())    plt.savefig("Artistic Image.png", dpi=200)def neuralstyle(epoch = 1000, show_period=100, image_size=(), learning_rate = 0.1, content_image = None, style_image = None, content_a = None, style_b = None,initial_noise_image=None, ctx = mx.gpu(0)):    #1. Data Preprocessing and noise data    content_image , style_image , noise_image = data_preprocessing(content_image = content_image, style_image = style_image, image_size=image_size ,ctx=ctx)    #initializing noise image below values    if initial_noise_image=="content_image":        noise_image = content_image    elif initial_noise_image=="style_image":        noise_image = style_image    if os.path.exists("vgg19.params"):        print("vgg19.params exists")        pretrained = mx.nd.load("vgg19.params")        W1_1=pretrained['arg:conv1_1_weight'].as_in_context(ctx)        B1_1=pretrained['arg:conv1_1_bias'].as_in_context(ctx)        W1_2=pretrained['arg:conv1_2_weight'].as_in_context(ctx)        B1_2=pretrained['arg:conv1_2_bias'].as_in_context(ctx)        W2_1=pretrained['arg:conv2_1_weight'].as_in_context(ctx)        B2_1=pretrained['arg:conv2_1_bias'].as_in_context(ctx)        W2_2=pretrained['arg:conv2_2_weight'].as_in_context(ctx)        B2_2=pretrained['arg:conv2_2_bias'].as_in_context(ctx)        W3_1=pretrained['arg:conv3_1_weight'].as_in_context(ctx)        B3_1=pretrained['arg:conv3_1_bias'].as_in_context(ctx)        W3_2=pretrained['arg:conv3_2_weight'].as_in_context(ctx)        B3_2=pretrained['arg:conv3_2_bias'].as_in_context(ctx)        W3_3=pretrained['arg:conv3_3_weight'].as_in_context(ctx)        B3_3=pretrained['arg:conv3_3_bias'].as_in_context(ctx)        W3_4=pretrained['arg:conv3_4_weight'].as_in_context(ctx)        B3_4=pretrained['arg:conv3_4_bias'].as_in_context(ctx)        W4_1=pretrained['arg:conv4_1_weight'].as_in_context(ctx)        B4_1=pretrained['arg:conv4_1_bias'].as_in_context(ctx)        W4_2=pretrained['arg:conv4_2_weight'].as_in_context(ctx)        B4_2=pretrained['arg:conv4_2_bias'].as_in_context(ctx)        W4_3=pretrained['arg:conv4_3_weight'].as_in_context(ctx)        B4_3=pretrained['arg:conv4_3_bias'].as_in_context(ctx)        W4_4=pretrained['arg:conv4_4_weight'].as_in_context(ctx)        B4_4=pretrained['arg:conv4_4_bias'].as_in_context(ctx)        W5_1=pretrained['arg:conv5_1_weight'].as_in_context(ctx)        B5_1=pretrained['arg:conv5_1_bias'].as_in_context(ctx)    else:        print("vgg19.params downloading")        url="http://data.dmlc.ml/models/imagenet/vgg/vgg19-0000.params"        urllib.request.urlretrieve(url,"vgg19.params")        print("vgg19.params downloading completed")        pretrained = mx.nd.load("vgg19.params")        W1_1=pretrained['arg:conv1_1_weight'].as_in_context(ctx)        B1_1=pretrained['arg:conv1_1_bias'].as_in_context(ctx)        W1_2=pretrained['arg:conv1_2_weight'].as_in_context(ctx)        B1_2=pretrained['arg:conv1_2_bias'].as_in_context(ctx)        W2_1=pretrained['arg:conv2_1_weight'].as_in_context(ctx)        B2_1=pretrained['arg:conv2_1_bias'].as_in_context(ctx)        W2_2=pretrained['arg:conv2_2_weight'].as_in_context(ctx)        B2_2=pretrained['arg:conv2_2_bias'].as_in_context(ctx)        W3_1=pretrained['arg:conv3_1_weight'].as_in_context(ctx)        B3_1=pretrained['arg:conv3_1_bias'].as_in_context(ctx)        W3_2=pretrained['arg:conv3_2_weight'].as_in_context(ctx)        B3_2=pretrained['arg:conv3_2_bias'].as_in_context(ctx)        W3_3=pretrained['arg:conv3_3_weight'].as_in_context(ctx)        B3_3=pretrained['arg:conv3_3_bias'].as_in_context(ctx)        W3_4=pretrained['arg:conv3_4_weight'].as_in_context(ctx)        B3_4=pretrained['arg:conv3_4_bias'].as_in_context(ctx)        W4_1=pretrained['arg:conv4_1_weight'].as_in_context(ctx)        B4_1=pretrained['arg:conv4_1_bias'].as_in_context(ctx)        W4_2=pretrained['arg:conv4_2_weight'].as_in_context(ctx)        B4_2=pretrained['arg:conv4_2_bias'].as_in_context(ctx)        W4_3=pretrained['arg:conv4_3_weight'].as_in_context(ctx)        B4_3=pretrained['arg:conv4_3_bias'].as_in_context(ctx)        W4_4=pretrained['arg:conv4_4_weight'].as_in_context(ctx)        B4_4=pretrained['arg:conv4_4_bias'].as_in_context(ctx)        W5_1=pretrained['arg:conv5_1_weight'].as_in_context(ctx)        B5_1=pretrained['arg:conv5_1_bias'].as_in_context(ctx)    #2. Predefined VGG19 Network    def vgg19(image):        # vgg19 - convolution part        conv1_1 = nd.Convolution(data=image, num_filter=64, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W1_1, bias=B1_1)        relu1_1 = nd.Activation(data=conv1_1, act_type='relu')        conv1_2 = nd.Convolution(data=relu1_1, num_filter=64, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W1_2, bias=B1_2)        relu1_2 = nd.Activation(data=conv1_2, act_type='relu')        pool1 = nd.Pooling(data=relu1_2, pad=(0, 0), kernel=(2, 2), stride=(2, 2), pool_type='avg')        conv2_1 = nd.Convolution(data=pool1, num_filter=128, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W2_1, bias=B2_1)        relu2_1 = nd.Activation( data=conv2_1, act_type='relu')        conv2_2 = nd.Convolution(data=relu2_1, num_filter=128, pad=(1, 1), kernel=(3, 3),stride=(1, 1), weight=W2_2, bias=B2_2)        relu2_2 = nd.Activation(data=conv2_2, act_type='relu')        pool2 = nd.Pooling( data=relu2_2, pad=(0, 0), kernel=(2, 2), stride=(2, 2),pool_type='avg')        conv3_1 = nd.Convolution(data=pool2, num_filter=256, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W3_1, bias=B3_1)        relu3_1 = nd.Activation(data=conv3_1, act_type='relu')        conv3_2 = nd.Convolution(data=relu3_1, num_filter=256, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W3_2, bias=B3_2)        relu3_2 = nd.Activation(data=conv3_2, act_type='relu')        conv3_3 = nd.Convolution(data=relu3_2, num_filter=256, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W3_3, bias=B3_3)        relu3_3 = nd.Activation( data=conv3_3, act_type='relu')        conv3_4 = nd.Convolution( data=relu3_3, num_filter=256, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W3_4, bias=B3_4)        relu3_4 = nd.Activation( data=conv3_4, act_type='relu')        pool3 = nd.Pooling(data=relu3_4, pad=(0, 0), kernel=(2, 2), stride=(2, 2), pool_type='avg')        conv4_1 = nd.Convolution(data=pool3, num_filter=512, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W4_1, bias=B4_1)        relu4_1 = nd.Activation(data=conv4_1, act_type='relu')        conv4_2 = nd.Convolution(data=relu4_1, num_filter=512, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W4_2, bias=B4_2)        relu4_2 = nd.Activation(data=conv4_2, act_type='relu')        conv4_3 = nd.Convolution(data=relu4_2, num_filter=512, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W4_3, bias=B4_3)        relu4_3 = nd.Activation(data=conv4_3, act_type='relu')        conv4_4 = nd.Convolution(data=relu4_3, num_filter=512, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W4_4, bias=B4_4)        relu4_4 = nd.Activation(data=conv4_4, act_type='relu')        pool4 = nd.Pooling(data=relu4_4, pad=(0, 0), kernel=(2, 2), stride=(2, 2), pool_type='avg')        conv5_1 = nd.Convolution(data=pool4, num_filter=512, pad=(1, 1), kernel=(3, 3), stride=(1, 1), weight=W5_1, bias=B5_1)        relu5_1 = nd.Activation(data=conv5_1, act_type='relu')        return relu1_1, relu2_1, relu3_1, relu4_1, relu5_1, relu4_2    #Adam optimizer    optimizer=mx.optimizer.Adam(rescale_grad=1, learning_rate=learning_rate)    state=[]    #learning parameter    params=[noise_image]    # attach gradient!!!    for param in params:        param.attach_grad()        state.append(optimizer.create_state(0, param))    #3 learning    for i in tqdm(range(1,epoch+1,1)):        c_loss=nd.array([0,],ctx=ctx)        s_loss=nd.array([0,],ctx=ctx)        with autograd.record(): #Location is very important            content=vgg19(content_image)            style=vgg19(style_image)            noise=vgg19(noise_image)            for j,(n,c,s) in enumerate(zip(noise,content,style), start=1):                batch_size, filter , height, width = n.shape                # (1)compute style lose                #using cov1_1 ,cov2_1 ,cov3_1 ,cov4_1 ,cov5_1                if j < len(content): # or len(noise) or len(style)                    #reshape                    n = n.reshape((-1, height*width))                    s = s.reshape((-1, height*width))                    N = filter                    M = height*width                    '''                    The style and noise size can be different                    -> This is because there are only the filter * filter(channel*channel) is left                    as a result of the gram matrix                    '''                    #gram_matrix                    n=nd.dot(n, n, transpose_a=False, transpose_b=True) #(filter, filter)                    s=nd.dot(s, s, transpose_a=False, transpose_b=True) #(filter, filter                    #autograd.record() is not supporting the += operator                    s_loss=s_loss+nd.mean(nd.multiply(nd.divide(nd.square(n-s),4*N*M),0.2)) #nd.mean((filter,))                    #s_loss = s_loss + nd.mean(nd.multiply(nd.divide(nd.square(n-s), 4 * (N*N) * (M*M)), 0.2)) # it is too small because of the square of N,M                # (2)compute content lose                #using conv4_2                if j == len(content): # or len(noise) or len(style)                    #reshape                    n = n.reshape((-1, height*width))                    c = c.reshape((-1, height*width))                    '''                    If you do not want the size of the content image and the noise image to be the same,                    you should work with c equal to n. i did not do anything here.                     Therefore, the content size and noise size should be the same.                     '''                    c_loss = nd.divide(nd.square(n-c),2)                    c_loss = nd.mean(c_loss)            c_loss = nd.multiply(c_loss, content_a)            s_loss = nd.multiply(s_loss, style_b)            loss = c_loss + s_loss        loss.backward()        for j,param in enumerate(params):            optimizer.update(0, param, param.grad, state[j])        print(" epoch : {} , cost : {}".format(i, loss.asscalar()))        #saving image        if i%show_period==0:            artistic_Image(noise_image,image_size)if __name__ == "__main__":    content_image = "content/tiger2.jpg"    style_image = "style/rain_princess.jpg"    initial_noise_image = "content_image"    image_size = (256, 512)    neuralstyle(epoch = 1000, show_period=100, image_size=image_size , learning_rate = 0.1, content_image = content_image, style_image = style_image, content_a = 1, style_b = 1000, initial_noise_image=initial_noise_image, ctx = mx.cpu(0))else:    print("Imported")