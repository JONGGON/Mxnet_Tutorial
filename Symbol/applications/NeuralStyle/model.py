import mxnet as mximport mxnet.ndarray as ndfrom algorithm1 import *#from algorithm2 import *from tqdm import *import matplotlib.pyplot as pltimport data_preprocessing as dpdef artistic_Image(noise_image,image_size):    image = noise_image.reshape((-1,)+image_size)    r,g,b = nd.split(image ,axis=0, num_outputs=3)    #Denormalization by JG    r= nd.multiply(r,0.229)+0.485    g= nd.multiply(g,0.224)+0.456    b= nd.multiply(b,0.225)+0.406    image=nd.concat(r,g,b,dim=0)    '''    matplotlib supports float32 and uint8 data types. For grayscale, matplotlib supports only float32.     If your array data does not meet one of these descriptions, you need to rescale it.    '''    image = nd.transpose(image, axes=(1, 2, 0))    image = nd.clip(image , a_min=0 , a_max=1)    image = nd.multiply(image,255)    image = nd.clip(image, a_min=0, a_max=255).astype('uint8')    plt.imshow(image.asnumpy())    plt.savefig("Artistic Image.png", dpi=300)def neuralstyle(epoch =None, show_period=None, learning_rate=None, image_size=None, content_image=None , style_image=None, content_a=None, style_b=None, initial_noise_image=None, ctx=None):    #1. Data Preprocessing and noise data    content_image, style_image, noise_image =dp.data_preprocessing(content_image = content_image, style_image = style_image, image_size=image_size ,ctx=ctx)    #initializing noise image below values    if initial_noise_image=="content_image":        noise_image=content_image    elif initial_noise_image=="style_image":        noise_image=style_image    else:        pass#noise_image=noise_image    #optimizer    optimizer = mx.optimizer.Adam(learning_rate=learning_rate)    state = optimizer.create_state(0,noise_image)    '''Binds the current symbol to an executor and returns it.    # content lose : using cov4_2    # style lose : using cov1_1 ,cov2_1 ,cov3_1 ,cov4_1 ,cov5_1        You only need to bind once.    With bind, you can change the argument values of args and args_grad. The value of a list or dictionary is mutable.    See the code below for an approach.    '''    total_loss = algorithm(content_a=content_a, style_b=style_b, content_image=content_image, style_image=style_image, noise_image=noise_image, image_size=image_size, ctx=ctx)    #2. learning    for i in tqdm(range(1,epoch+1,1)):        #(1-1) When the output of the network is not a loss function.        #total_cost.arg_dict["noise"]=noise_image        #output=total_loss.forward(is_train=True)        #total_loss.backward(output[0])        #Related explanation.        """Do backward pass to get the gradient of arguments.        Parameters        ----------        out_grads : NDArray or list of NDArray or dict of str to NDArray, optional            Gradient on the outputs to be propagated back.            This parameter is only needed when bind is called            on outputs that are not a loss function.                    is_train : bool, default True            Whether this backward is for training or inference. Note that in rare            cases you want to call backward with is_train=False to get gradient            during inference.        Examples        --------        >>> # Example for binding on loss function symbol, which gives the loss value of the model.        >>> # Equivalently it gives the head gradient for backward pass.        >>> # In this example the built-in SoftmaxOutput is used as loss function.        >>> # MakeLoss can be used to define customized loss function symbol.        >>> net = mx.sym.Variable('data')        >>> net = mx.sym.FullyConnected(net, name='fc', num_hidden=6)        >>> net = mx.sym.Activation(net, name='relu', act_type="relu")        >>> net = mx.sym.SoftmaxOutput(net, name='softmax')        >>> args =  {'data': mx.nd.ones((1, 4)), 'fc_weight': mx.nd.ones((6, 4)),        >>>          'fc_bias': mx.nd.array((1, 4, 4, 4, 5, 6)), 'softmax_label': mx.nd.ones((1))}        >>> args_grad = {'fc_weight': mx.nd.zeros((6, 4)), 'fc_bias': mx.nd.zeros((6))}        >>> texec = net.bind(ctx=mx.cpu(), args=args, args_grad=args_grad)        >>> out = texec.forward(is_train=True)[0].copy()        >>> print out.asnumpy()        [[ 0.00378404  0.07600445  0.07600445  0.07600445  0.20660152  0.5616011 ]]        >>> texec.backward()        >>> print(texec.grad_arrays[1].asnumpy())        [[ 0.00378404  0.00378404  0.00378404  0.00378404]         [-0.92399555 -0.92399555 -0.92399555 -0.92399555]         [ 0.07600445  0.07600445  0.07600445  0.07600445]         [ 0.07600445  0.07600445  0.07600445  0.07600445]         [ 0.20660152  0.20660152  0.20660152  0.20660152]         [ 0.5616011   0.5616011   0.5616011   0.5616011 ]]        >>>        >>> # Example for binding on non-loss function symbol.        >>> # Here the binding symbol is neither built-in loss function        >>> # nor customized loss created by MakeLoss.        >>> # As a result the head gradient is not automatically provided.        >>> a = mx.sym.Variable('a')        >>> b = mx.sym.Variable('b')        >>> # c is not a loss function symbol        >>> c = 2 * a + b        >>> args = {'a': mx.nd.array([1,2]), 'b':mx.nd.array([2,3])}        >>> args_grad = {'a': mx.nd.zeros((2)), 'b': mx.nd.zeros((2))}        >>> texec = c.bind(ctx=mx.cpu(), args=args, args_grad=args_grad)        >>> out = texec.forward(is_train=True)[0].copy()        >>> print(out.asnumpy())        [ 4.  7.]        >>> # out_grads is the head gradient in backward pass.        >>> # Here we define 'c' as loss function.        >>> # Then 'out' is passed as head gradient of backward pass.        >>> texec.backward(out)        >>> print(texec.grad_arrays[0].asnumpy())        [ 8.  14.]        >>> print(texec.grad_arrays[1].asnumpy())        [ 4.  7.]        """        '''        noise_image takes its value in bind and updates the noise_image in the update function         (noise_image is kept as one value (because it is mutable type))        Python's 'list type' can be exchanged with the pointer type of the c language.        '''        #(1-2) When the output of the network is a loss function.        total_loss.forward(is_train=True)        total_loss.backward()        #noise_image is continuously updated because it is a mutable variable.        optimizer.update(0,noise_image, total_loss.grad_dict["noise_"], state)        #optimizer.update(0,noise_image,total_loss.grad_arrays[0], state)        print(" epoch : {} , cost : {}".format(i, total_loss.outputs[0].asscalar()))        #saving image        if i%show_period==0:            artistic_Image(noise_image,image_size)if __name__ == "__main__":    # content_a  / style_b = 1/1000    content_image = "content/tiger.jpg"    style_image = "style/picasso.jpg"    initial_noise_image = "content_image"  # or style image or noise -> Assigning an initial value to the content image is faster than assigning noise.    image_size = (256, 512)  # height , width -> is expected to be at least 224.    neuralstyle(epoch=1000, show_period=100, learning_rate=0.1, image_size=image_size,                               content_image=content_image, style_image=style_image, content_a=1, style_b=1000,                               initial_noise_image=initial_noise_image, ctx=mx.gpu(0))else:    print("Imported")